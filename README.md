
# Welcome to my GitHub profile
ğŸŒŸI am a graduate student at the University of Central Missouri, focusing on cloud-based data engineering and entry-level machine learning workflows. My work involves building scalable data pipelines, experimenting with cloud services, and developing Python-based tools for analytics and automation.

I have hands-on experience with AWS (SageMaker, Glue, Lambda, Redshift) and exposure to GCP (BigQuery), along with practical skills in Python, SQL, PySpark, FastAPI, and Power BI. This GitHub includes projects related to cloud data workflows, basic ML development, feature engineering, ETL processes, API development, and serverless automation.

I am continuously working on improving my technical skills and developing real-world projects as I grow in the field of cloud and data engineering.s.
---

## ğŸš€ About Me

- ğŸ“ Passionate about solving real-world problems using data.
- â˜ Hands-on experience building cloud data pipelines using AWS (S3, Glue, RDS).
- ğŸ›  Skilled in Python, SQL, PySpark, and Business Intelligence tools like Power BI.
- ğŸ“ˆ Interested in turning raw data into actionable business insights.
- ğŸ’¬ Always learning and growing in the world of data!

---

## ğŸ›  Skills and Tools


Programming: Python, SQL, PySpark, Bash
AI/ML & NLP: Transformers, Hugging Face, LangChain, spaCy, OpenAI API, PyTorch, TensorFlow, scikit-learn, XGBoost
Data Engineering: PySpark, Airflow, Kafka, ETL, Delta Lake, Data Lakes, Data Warehouses
Web & API Development: FastAPI, Flask, Django, RESTful APIs
Cloud Platforms: AWS (Primary), Azure (Basic), GCP (BigQuery)
MLOps & DevOps: Docker, Kubernetes, GitHub Actions, CI/CD, MLflow, Terraform (Basics)
Databases: PostgreSQL, ElasticSearch, Pinecone, Snowflake
Visualization: Power BI, Tableau, Matplotlib
Other Tools/Concepts: GitHub, Linux, Agile/Scrum, Prompt Engineering, RAG Architecture, Vector Databases

---

## ğŸ“šHands-ON Projects

- Customer Purchase Insights Dashboard:-
  Built an End-to-End Aws data pipeline to extract raw customer purchase data from S3, transform it using AWS Glue (PySpark), and load into PostgreSQL RDS database. Used S3, Glue Crawler, Glue ETL, RDS PostgreSQL, and psql CLI to enable structured data ready for business reporting and dashboards. 

- Smart Data Lake:-
  Smart Data Lake Builder is an automation platform that simplifies data loading and transformation. It may be used to build data pipelines for a variety of use cases, including Lakehouse, Data Products, and Data Mesh. SDLB is written in Java/Scala and is based on open-source big data technologies such as Apache Hadoop and Apache Spark. It has connectors for a variety of data sources (HadoopFS, Hive, DeltaLake) and file formats.

- Employee Leave Management System
  Worked with team of 2. The main theme of the project is making leave application process easier. The login form was created using HTML and PHP and then developed a website using CSS code.

## ğŸ“« Let's Connect

- [LinkedIn Profile] https://www.linkedin.com/in/nandhini-cr-8b1678251/ 
- ğŸ“§ Email: crnandhini5@gmail.com

---

# ğŸŒŸ Thank you for visiting myÂ GitHubÂ Profile!



